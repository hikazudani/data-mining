{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ed6fd1",
   "metadata": {},
   "source": [
    "### Summary of Alternative Classification Techniques\n",
    "\n",
    "This document details several alternative methods for data classification beyond decision trees, focusing on rule-based classifiers, nearest-neighbor classifiers, and Bayesian classifiers.\n",
    "\n",
    "#### Rule-Based Classifiers\n",
    "\n",
    "This technique uses a set of \"if-then\" rules to classify records. A rule consists of an *antecedent* (the \"if\" part, containing attribute tests) and a *consequent* (the \"then\" part, which assigns a class).\n",
    "\n",
    "* **Rule Quality:** Rules are evaluated using two main metrics:\n",
    "    * **Coverage:** The fraction of records in the dataset that satisfy the rule's antecedent.\n",
    "    * **Precision:** The fraction of records covered by the rule that actually belong to the predicted class.\n",
    "\n",
    "* **Conflict Resolution:** A dataset can present challenges if a record is covered by no rules (incomplete set) or by multiple rules with conflicting classes (not mutually exclusive). There are two primary ways to resolve this:\n",
    "    * **Ordered Rules (Decision List):** Rules are ranked by priority (e.g., by precision). The highest-priority rule that fires is used to classify the record.\n",
    "    * **Unordered Rules:** All rules that fire are allowed to \"vote\" for their respective classes. The class with the most votes (potentially weighted by rule precision) is assigned.\n",
    "\n",
    "* **Rule Extraction Methods:**\n",
    "    * **Direct Methods:** Rules are extracted directly from the data. The most common approach is the **sequential covering algorithm**. This algorithm iteratively finds the single \"best\" rule (e.g., highest precision or information gain), adds it to the list, removes the training records covered by that rule, and repeats the process on the remaining data. The **RIPPER** algorithm is a well-known implementation of this method.\n",
    "    * **Indirect Methods:** Rules are extracted from another model. For example, a decision tree can be converted into a rule set, where each path from the root to a leaf becomes a rule. These rules are often then pruned and simplified to improve generality.\n",
    "\n",
    "#### Bayesian Classifiers\n",
    "\n",
    "These classifiers use probability to model the non-deterministic relationship between attributes and a class, based on **Bayes' Theorem**.\n",
    "\n",
    "* **Core Concept:** The goal is to find the class $Y$ that has the highest *posterior probability* $P(Y|X)$, given the attributes $X$.\n",
    "* **Bayes' Theorem:** The posterior probability is calculated as:\n",
    "    $P(Y|X) = [P(X|Y) \\times P(Y)] / P(X)$\n",
    "    * $P(Y)$: The *prior probability* of the class (its frequency in the training data).\n",
    "    * $P(X|Y)$: The *class conditional probability* (the probability of observing attributes $X$ *given* that the class is $Y$).\n",
    "    * $P(X)$: The probability of the attributes (acts as a normalizing constant and can be ignored for classification).\n",
    "\n",
    "##### Naive Bayes Classifier\n",
    "\n",
    "Directly calculating $P(X|Y)$ is computationally difficult. The **Naive Bayes** classifier simplifies this by making a \"naive\" assumption of **conditional independence**â€”it assumes that all attributes are independent of each other, given the class.\n",
    "\n",
    "* **Simplified Calculation:** This assumption allows the class conditional probability to be calculated as the product of the individual probabilities for each attribute:\n",
    "    $P(X|Y) = P(X_1|Y) \\times P(X_2|Y) \\times ... \\times P(X_d|Y)$\n",
    "\n",
    "* **Handling Attribute Types:**\n",
    "    * **Categorical Attributes:** Probabilities are estimated as the simple fraction of times that attribute value appears with that class in the training data (e.g., $P(\\text{Marital Status} = \\text{Single} | \\text{Class} = \\text{No}) = 2/7$).\n",
    "    * **Continuous Attributes:** Typically handled by assuming the attribute follows a **Gaussian (Normal) distribution**. The algorithm calculates the mean ($\\mu$) and variance ($\\sigma^2$) for that attribute for each class, and then uses the Gaussian probability density function to find $P(X_i|Y)$.\n",
    "\n",
    "* **m-Estimate (Laplace Correction):** This is a crucial technique to handle zero-probability issues. If a specific attribute-value pair never appears in the training set for a class (e.g., $P(\\text{Marital Status} = \\text{Married} | \\text{Class} = \\text{Yes}) = 0$), it would cause the entire posterior probability to become zero. The m-estimate adds a small number of \"virtual\" examples to prevent this.\n",
    "\n",
    "* **Characteristics:** Naive Bayes is robust to noise and irrelevant attributes. However, its performance can suffer if the attributes are highly correlated, as this violates its fundamental assumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbeecf9",
   "metadata": {},
   "source": [
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem is a statistical principle that combines prior knowledge with new evidence gathered from data.\n",
    "\n",
    "Given two random variables, X and Y, their relationship can be described by:\n",
    "* **Joint Probability, $P(X,Y)$:** The probability that X takes a specific value *and* Y takes a specific value.\n",
    "* **Conditional Probability, $P(Y|X)$:** The probability that Y takes a specific value *given that* X's value is known.\n",
    "\n",
    "These probabilities are linked by the formula:\n",
    "$$P(X,Y) = P(Y|X) \\times P(X) = P(X|Y) \\times P(Y)$$\n",
    "\n",
    "By manipulating this relationship, Bayes' Theorem is derived. One form is presented as:\n",
    "$$P(Y|X) = \\frac{P(X,Y)}{P(X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb2779f",
   "metadata": {},
   "source": [
    "### Summary of Naive Bayes Classification Problem\n",
    "\n",
    "This document presents a classification problem solved using the Naive Bayes algorithm. The goal is to build a predictive model to estimate whether a third-year college student will graduate within five years (\"D\" - within the deadline) or take longer (\"F\" - outside the deadline).\n",
    "\n",
    "The model is built using a dataset of 10 graduated students. The attributes, all of which can be \"A\" or \"B,\" are:\n",
    "* **DD:** Performance in courses.\n",
    "* **DA:** Performance in complementary activities.\n",
    "* **DE:** Performance in internships.\n",
    "\n",
    "The solution demonstrates the steps to build the model and classify new instances.\n",
    "\n",
    "#### a) Building the Naive Bayes Model\n",
    "\n",
    "1.  **Prior Probabilities (P(Class)):**\n",
    "    * $P(D) = 6/10$\n",
    "    * $P(F) = 4/10$\n",
    "\n",
    "2.  **Class Conditional Probabilities (P(Attribute=Value | Class)):**\n",
    "    * $P(DD=A|D) = 4/6$\n",
    "    * $P(DD=B|D) = 2/6$\n",
    "    * $P(DA=A|D) = 3/6$\n",
    "    * $P(DA=B|D) = 3/6$\n",
    "    * $P(DE=A|D) = 4/6$\n",
    "    * $P(DE=B|D) = 2/6$\n",
    "    * $P(DD=A|F) = 1/4$\n",
    "    * $P(DD=B|F) = 3/4$\n",
    "    * $P(DA=A|F) = 3/4$\n",
    "    * $P(DA=B|F) = 1/4$\n",
    "    * $P(DE=A|F) = 2/4$\n",
    "    * $P(DE=B|F) = 2/4$\n",
    "\n",
    "#### b) Classification Test for Student $X = (DD=A, DA=B, DE=B)$\n",
    "\n",
    "1.  **Calculate $P(X | Class)$:**\n",
    "    * $P(X|D) = P(DD=A|D) \\times P(DA=B|D) \\times P(DE=B|D) = (4/6) \\times (3/6) \\times (2/6) = 1/9$\n",
    "    * $P(X|F) = P(DD=A|F) \\times P(DA=B|F) \\times P(DE=B|F) = (1/4) \\times (1/4) \\times (2/4) = 1/32$\n",
    "\n",
    "2.  **Calculate Posterior Probability $P(Class | X)$:**\n",
    "    * $P(D|X) = P(D) \\times P(X|D) = (6/10) \\times (1/9) = 1/15$\n",
    "    * $P(F|X) = P(F) \\times P(X|F) = (4/10) \\times (1/32) = 1/80$\n",
    "\n",
    "**Result:** Since $1/15 > 1/80$, the student (A, B, B) is classified as \"D\" (will graduate within the deadline).\n",
    "\n",
    "#### c) Classification Test for Student (B, B, B)\n",
    "\n",
    "The student is classified as \"F\" (outside the deadline) because $P(F|X) = 0.0375$ is greater than $P(D|X) = 0.03$.\n",
    "\n",
    "#### d) Classification Test for Student (A, A, A)\n",
    "\n",
    "The student is classified as \"D\" (within the deadline) because $P(D|X) = 0.13$ is greater than $P(F|X) = 0.0375$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
