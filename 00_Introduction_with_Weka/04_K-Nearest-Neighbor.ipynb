{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "446586df",
   "metadata": {},
   "source": [
    "### Summary of the K-Nearest Neighbors (k-NN) Algorithm\n",
    "\n",
    "The k-nearest neighbors (k-NN) algorithm is a form of **instance-based learning**. This means it does not build a predictive model in advance. Instead, it classifies a new, unknown item by storing the entire training dataset and comparing the new item to every stored item.\n",
    "\n",
    "This method is known as **\"lazy evaluation\"** because all the computation is deferred until a classification is requested. While the logic is simple, this process can be computationally expensive if the training dataset is large.\n",
    "\n",
    "#### The Classification Process\n",
    "\n",
    "The core principle of k-NN is **\"majority vote.\"** When a new test item is presented, the algorithm performs the following steps:\n",
    "\n",
    "1.  **Calculate Distances:** It computes the distance between the test item and *every* item in the stored training dataset.\n",
    "2.  **Find Neighbors:** It identifies the *k* items from the training set that have the shortest distances (i.e., the \"k-nearest neighbors\").\n",
    "3.  **Vote for Class:** The test item is assigned the class that is most common (the majority) among those *k* neighbors.\n",
    "\n",
    "#### Distance Metrics\n",
    "\n",
    "To find the \"nearest\" neighbors, a distance metric is required. This metric treats each attribute as a dimension in space and each data item as a point. The choice of metric is important and must be appropriate for the data.\n",
    "\n",
    "The most common metric is the **Euclidean distance**, which represents the physical, straight-line distance between two points (the hypotenuse in a right triangle). For two items, $\\vec{x}_{p}$ and $\\vec{x}_{q}$, in a *d*-dimensional space, the formula is:\n",
    "\n",
    "$$dist_{\\vec{x}_{p},\\vec{x}_{q}}=\\sqrt{\\sum_{j=1}^{d}(x_{pj}-x_{qj})^{2}}$$\n",
    "\n",
    "#### Parameter Selection (k)\n",
    "\n",
    "The algorithm requires the parameter *k* (the number of neighbors) to be defined. This choice must be made carefully:\n",
    "* **Small *k***: Can make the algorithm overly sensitive to noise.\n",
    "* **Large *k***: Can be computationally intensive and may dilute the classification by including neighbors from different, irrelevant classes.\n",
    "\n",
    "#### Example Application\n",
    "\n",
    "An example is provided using a \"Satisfaction Survey\" dataset, which classifies customers as satisfied (SAT) or unsatisfied (INS) based on normalized \"Time\" and \"Price\" attributes.\n",
    "\n",
    "* **Test Item:** $\\vec{x}_{1}$ (Time=0.70, Price=0.40)\n",
    "* **Parameters:** *k*=3, Euclidean distance.\n",
    "* **Process:**\n",
    "    1.  The algorithm calculates the distance from $\\vec{x}_{1}$ to all other points.\n",
    "    2.  It sorts the results and finds the 3-nearest neighbors:\n",
    "        * $\\vec{x}_{3}$ (distance 0.09, class: **SAT**)\n",
    "        * $\\vec{x}_{10}$ (distance 0.10, class: **INS**)\n",
    "        * $\\vec{x}_{5}$ (distance 0.14, class: **SAT**)\n",
    "    3.  The algorithm holds a majority vote. The classes of the neighbors are {SAT, INS, SAT}.\n",
    "* **Result:** With two votes for \"SAT\" and one for \"INS,\" the test item $\\vec{x}_{1}$ is classified as **SAT**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462594d",
   "metadata": {},
   "source": [
    "### Summary of a K-NN Classification for Facial Recognition\n",
    "\n",
    "This document outlines a facial recognition classification problem solved using the K-NN algorithm. The process involves extracting characteristic points from a digital image to create a dataset of face measurements. A sample dataset is provided with four examples, each with four attributes (Left Eye, Right Eye, Nose, Mouth) and a class label (F = Face, N = Not Face).\n",
    "\n",
    "The task is to classify a new, unknown example, $x_1 = (7, 6, 5, 4)$, using the K-NN algorithm with K=3 and Euclidean distance as the metric.\n",
    "\n",
    "The Euclidean distances between the new example ($x_1$) and the four known examples in the dataset are calculated:\n",
    "* **Distance to $x_2 = (6, 6, 1, 2)$ [Class F]:** 4.58\n",
    "* **Distance to $x_3 = (1, 3, 3, 4)$ [Class F]:** 7.00\n",
    "* **Distance to $x_4 = (2, 10, 8, 4)$ [Class N]:** 7.07\n",
    "* **Distance to $x_5 = (8, 8, 6, 4)$ [Class N]:** 2.45\n",
    "\n",
    "To classify the new example with K=3, the three nearest neighbors are identified by their distances:\n",
    "1.  $x_5$ (Distance = 2.45, Class = N)\n",
    "2.  $x_2$ (Distance = 4.58, Class = F)\n",
    "3.  $x_3$ (Distance = 7.00, Class = F)\n",
    "\n",
    "Based on a majority vote of the three neighbors (two \"F\" and one \"N\"), the new example $x_1$ is classified as F (Face)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
